{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"Jacaranda/UlizaLlama\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig()\n",
    "\n",
    "# Load the quantized model\n",
    "model_quantized = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "\n",
    "# Print model details\n",
    "print(model_quantized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_text = \"Hapo zamani za kale\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "inputs = {k: v.to('cuda') for k, v in inputs.items()}  # Move input tensors to CUDA\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Question Answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = \"Mji mkuu wa Kenya ni upi?\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "input_text = \"Your long text here\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pipelines for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Text Generation Pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generated_text = text_generator(\"Hapo zamani za kale\", max_length=50)\n",
    "print(generated_text)\n",
    "\n",
    "# Translation Pipeline \n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "translated_text = translator(\"Hello, how are you?\")\n",
    "print(translated_text)\n",
    "\n",
    "# Question Answering Pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "answer = question_answerer(question=\"Mji mkuu wa Kenya ni upi?\", context=\"Kenya ni nchi katika Afrika Mashariki. Mji mkuu ni Nairobi.\")\n",
    "print(answer)\n",
    "\n",
    "# Summarization Pipeline \n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "summary = summarizer(\"Your long text here\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
